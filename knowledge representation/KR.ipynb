{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e08244-3f3f-46ad-b4a6-430c6b123423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d90b952-bf08-44d4-9d3f-3d4efc78192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from rdflib import Graph, Namespace, RDF, RDFS, OWL, Literal\n",
    "    HAVE_RDFLIB = True\n",
    "except ImportError:\n",
    "    HAVE_RDFLIB = False\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "RAW_PATH = 'C:\\\\Users\\\\user\\\\Downloads\\\\Sentosa-main - Copy\\\\Sentosa-main\\\\scraper\\\\raw_text_data.json'     \n",
    "CLEAN_PATH = 'C:\\\\Users\\\\user\\\\Downloads\\\\Sentosa-main - Copy\\\\Sentosa-main\\\\preprocessing\\\\clean_text_data.json' \n",
    "\n",
    "OUTPUT_DIR = 'kr_output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.isfile(RAW_PATH):\n",
    "    logger.error(f\"File not found: {RAW_PATH}. Please verify the path.\")\n",
    "    raise FileNotFoundError(f\"{RAW_PATH} not found\")\n",
    "if not os.path.isfile(CLEAN_PATH):\n",
    "    logger.error(f\"File not found: {CLEAN_PATH}. Please verify the path.\")\n",
    "    raise FileNotFoundError(f\"{CLEAN_PATH} not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "077e1a80-012e-4a1b-93f3-b2f0494d5e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:02:53] INFO: raw and clean entry counts match: 122\n"
     ]
    }
   ],
   "source": [
    "with open(RAW_PATH, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "with open(CLEAN_PATH, 'r', encoding='utf-8') as f:\n",
    "    clean_data = json.load(f)\n",
    "\n",
    "len_raw = len(raw_data)\n",
    "len_clean = len(clean_data)\n",
    "if len_raw != len_clean:\n",
    "    logger.warning(f\"raw ({len_raw}) and clean ({len_clean}) entry counts do not match. Please verify alignment.\")\n",
    "else:\n",
    "    logger.info(f\"raw and clean entry counts match: {len_raw}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20de0d42-68c2-4392-b689-a5bf959f1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:02:54] INFO: [0] raw title: 'Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!'\n",
      "[19:02:54] INFO:      clean_title tokens: [['anxiety', 'bantu', 'tenang']]\n",
      "[19:02:54] INFO: [1] raw title: 'Ubat Anxiety: Ini 5 Jenis Ubat Boleh Beli Tapi Mesti Dengan Surat Doktor!'\n",
      "[19:02:54] INFO:      clean_title tokens: [['ubat', 'anxiety', 'jenis', 'ubat', 'beli', 'surat', 'doktor']]\n",
      "[19:02:54] INFO: [2] raw title: 'Punca Penyakit Anxiety: Ini 7 Perkara Boleh Trigger Anxiety Anda!'\n",
      "[19:02:54] INFO:      clean_title tokens: [['punca', 'sakit', 'anxiety', 'trigger', 'anxiety']]\n"
     ]
    }
   ],
   "source": [
    "# Show first few entries for sanity check\n",
    "for i in range(min(3, len_raw)):\n",
    "    rt = raw_data[i].get('title', '')\n",
    "    ct = clean_data[i].get('clean_title', [[]])\n",
    "    logger.info(f\"[{i}] raw title: {rt!r}\")\n",
    "    logger.info(f\"     clean_title tokens: {ct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361a574b-7504-4811-9b34-95518cda8f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = {\n",
    "    # Anxiety related\n",
    "    'anxiety attack': 'serangan kegelisahan',\n",
    "    'panic attack': 'serangan panik',\n",
    "    'anxiety': 'kegelisahan',\n",
    "    'kecemasan': 'kegelisahan',\n",
    "    'kebimbangan': 'kegelisahan',\n",
    "    'kerisauan': 'kegelisahan',\n",
    "    # Stress related\n",
    "    'stress': 'tekanan emosi',\n",
    "    'stres': 'tekanan emosi',\n",
    "    # Depression related\n",
    "    'depression': 'depresi',\n",
    "    'depresi': 'depresi',\n",
    "    # PTSD\n",
    "    'ptsd': 'stres pasca trauma',\n",
    "    'stres pasca trauma': 'stres pasca trauma',\n",
    "    # Panic\n",
    "    'panic': 'serangan panik',\n",
    "    'serangan panik': 'serangan panik',\n",
    "    # OCD\n",
    "    'ocd': 'gangguan obsesif kompulsif',\n",
    "    'obsesif kompulsif': 'gangguan obsesif kompulsif',\n",
    "    # Insomnia\n",
    "    'insomnia': 'insomnia',\n",
    "    # Fobia examples, add more as needed\n",
    "    'claustrophobia': 'fobia klaustrofobia',\n",
    "    'fobia badut': 'fobia badut',\n",
    "    # Add more terms as discovered\n",
    "}\n",
    "\n",
    "synonym_map = {\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "nodes = []        \n",
    "edges = []        \n",
    "entity_to_id = {}  \n",
    "entity_counters = {\n",
    "    'Condition': 0,\n",
    "    'Symptom': 0,\n",
    "    'Trigger': 0,\n",
    "    'Treatment': 0,\n",
    "    'Strategy': 0,\n",
    "    'Professional': 0,\n",
    "}\n",
    "\n",
    "condition_definitions = {}\n",
    "\n",
    "unhandled_entries = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8737597d-07e2-4218-b558-80ffc73b8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entity(phrase: str) -> str:\n",
    "    p = phrase.lower().strip()\n",
    "    p = re.sub(r'\\s+', ' ', p)\n",
    "    for key in sorted(normalization.keys(), key=lambda x: -len(x)):\n",
    "        val = normalization[key]\n",
    "        pattern = r'\\b' + re.escape(key) + r'\\b'\n",
    "        if re.search(pattern, p):\n",
    "            p = re.sub(pattern, val, p)\n",
    "    for syn, can in synonym_map.items():\n",
    "        pattern = r'\\b' + re.escape(syn) + r'\\b'\n",
    "        if re.search(pattern, p):\n",
    "            p = re.sub(pattern, can, p)\n",
    "    p = re.sub(r'\\s+', ' ', p).strip()\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aaeb0ef-dfe9-40e4-a6c3-a63eb0b3b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_entity(norm_phrase: str, ent_type: str) -> str:\n",
    "    key = f\"{ent_type}|{norm_phrase}\"\n",
    "    if key in entity_to_id:\n",
    "        return entity_to_id[key]\n",
    "    entity_counters[ent_type] += 1\n",
    "    prefix = {\n",
    "        'Condition': 'C',\n",
    "        'Symptom': 'S',\n",
    "        'Trigger': 'T',\n",
    "        'Treatment': 'TRT',\n",
    "        'Strategy': 'STR',\n",
    "        'Professional': 'PRO'\n",
    "    }.get(ent_type, 'E')\n",
    "    new_id = f\"{prefix}{entity_counters[ent_type]:03d}\"\n",
    "    entity_to_id[key] = new_id\n",
    "    nodes.append({\n",
    "        'id': new_id,\n",
    "        'label_malay': norm_phrase,\n",
    "        'label_english': '',\n",
    "        'type': ent_type\n",
    "    })\n",
    "    return new_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55820f88-1745-4435-912d-5d1b9ca44a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_definition(raw_content: str) -> str:\n",
    "    text = re.sub(r'<[^>]+>', '', raw_content).strip()\n",
    "    parts = re.split(r'\\n\\n|\\r\\n\\r\\n', text)\n",
    "    first_para = parts[0] if parts else text\n",
    "    sents = re.split(r'(?<=[\\.！？\\?])\\s+', first_para)\n",
    "    definition = ' '.join(sents[:2]).strip()\n",
    "    if len(definition) < 20 and len(sents) > 2:\n",
    "        definition = ' '.join(sents[:3]).strip()\n",
    "    return definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da765451-798c-4b9a-b561-1437bfd3a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list_after_keyword(content: str, keyword_patterns: list) -> list:\n",
    "    text = re.sub(r'<[^>]+>', '', content).strip()\n",
    "    for pat in keyword_patterns:\n",
    "        match = re.search(pat, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            after = text[match.end():]\n",
    "            parts = re.split(r'\\n\\n|\\r\\n\\r\\n', after)\n",
    "            segment = parts[0] if parts else after\n",
    "            items = re.split(r'\\n|\\r\\n|,|;|\\d+\\.\\s*|\\s+dan\\s+', segment)\n",
    "            cleaned = []\n",
    "            for item in items:\n",
    "                item = item.strip().strip('.').strip()\n",
    "                if not item or len(item) < 3:\n",
    "                    continue\n",
    "                if re.search(r'\\b(apa|bagaimana|mengapa)\\b', item.lower()):\n",
    "                    continue\n",
    "                cleaned.append(item)\n",
    "            return cleaned\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d9e78b-31f1-4a1a-ae2d-5e87b0b1f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_condition_from_title(raw_title: str) -> str:\n",
    "    title = raw_title.strip()\n",
    "    t = title.lower()\n",
    "\n",
    "    # 1) Definition patterns\n",
    "    m = re.search(r'\\b(?:apa itu|apa yang harus anda tahu|perlu tahu|info tentang|ketahui lebih lanjut)\\s+(?:penyakit\\s+|gangguan\\s+)?(.+?)[\\?|:]', title, re.IGNORECASE)\n",
    "    if m:\n",
    "        cond = m.group(1).strip()\n",
    "    else:\n",
    "        # 2) Common patterns\n",
    "        patterns = [\n",
    "            r'(?:Simptom|Tanda(?: Umum)?|Gejala)\\s+(.+?):',\n",
    "            r'(?:Punca Penyakit|Punca)\\s+(.+?):',\n",
    "            r'(?:Ubat|Rawat|Cara Mengatasi|Cara Atasi|Terapi|Kaunseling|Pantang Larang)\\s+(.+?):',\n",
    "            r'(?:Penyakit|Gangguan)\\s+(.+?):',\n",
    "        ]\n",
    "        cond = None\n",
    "        for pat in patterns:\n",
    "            match = re.search(pat, title, re.IGNORECASE)\n",
    "            if match:\n",
    "                cond = match.group(1).strip()\n",
    "                cond = re.sub(r'\\b(ini|boleh|untuk).+', '', cond, flags=re.IGNORECASE).strip()\n",
    "                break\n",
    "        if cond is None:\n",
    "            # 3) Before comma\n",
    "            m2 = re.match(r'^\\s*([^,，]+)', title)\n",
    "            if m2:\n",
    "                part = m2.group(1).strip()\n",
    "                cond = part\n",
    "            else:\n",
    "                # 4) '&' or 'dan' multiple conditions: take first\n",
    "                if '&' in title:\n",
    "                    first = title.split('&')[0].strip()\n",
    "                    cond = first\n",
    "                elif ' dan ' in t:\n",
    "                    first = title.lower().split(' dan ')[0].strip()\n",
    "                    cond = first\n",
    "                else:\n",
    "                    # 5) Fallback: before '?' or ':' last word\n",
    "                    if '?' in title:\n",
    "                        before = title.split('?', 1)[0]\n",
    "                    elif ':' in title:\n",
    "                        before = title.split(':', 1)[0]\n",
    "                    else:\n",
    "                        before = title\n",
    "                    tokens = re.findall(r'\\b\\w+\\b', before.strip())\n",
    "                    cond = tokens[-1] if tokens else before.strip()\n",
    "    # 6) Remove prefixes 'gangguan ' / 'penyakit '\n",
    "    cond_lower = cond.lower().strip()\n",
    "    for prefix in ['gangguan ', 'penyakit ']:\n",
    "        if cond_lower.startswith(prefix):\n",
    "            cond = cond[len(prefix):].strip()\n",
    "            break\n",
    "    return cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb713e7a-54dd-4452-8221-458889900a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_relation(clean_title_tokens: list, raw_title: str) -> str:\n",
    "    toks = [tok.lower() for tok in clean_title_tokens]\n",
    "    t = raw_title.lower()\n",
    "\n",
    "    # 1) Definition patterns\n",
    "    if any([\n",
    "        'apa itu' in t,\n",
    "        re.search(r'\\bapa yang harus anda tahu\\b', t),\n",
    "        re.search(r'\\bperlu tahu\\b', t),\n",
    "        re.search(r'ketahui lebih lanjut', t),\n",
    "        re.search(r'info tentang', t),\n",
    "    ]):\n",
    "        return 'definition'\n",
    "\n",
    "    # 2) 'Cara Atasi' implies treated_by\n",
    "    if 'cara atasi' in t or 'lakukan cara ini' in t:\n",
    "        return 'treated_by'\n",
    "\n",
    "    # 3) has_symptom patterns\n",
    "    if any(tok in toks for tok in ['simptom', 'tanda', 'gejala']):\n",
    "        return 'has_symptom'\n",
    "    if re.search(r'\\b(simptom|tanda|gejala)\\b', t):\n",
    "        return 'has_symptom'\n",
    "\n",
    "    # 4) triggered_by patterns\n",
    "    if any(tok in toks for tok in ['punca', 'faktor', 'picu']):\n",
    "        return 'triggered_by'\n",
    "    if re.search(r'\\b(punca|faktor|picu)\\b', t):\n",
    "        return 'triggered_by'\n",
    "\n",
    "    # 5) treated_by patterns: ubat/rawat/terapi/kaunseling/pantang larang\n",
    "    if any(tok in toks for tok in ['ubat', 'rawat', 'cara', 'terapi', 'kaunseling', 'pantang', 'larang']):\n",
    "        return 'treated_by'\n",
    "    if re.search(r'\\b(ubat|rawat|cara mengatasi|terapi|kaunseling|pantang larang)\\b', t):\n",
    "        return 'treated_by'\n",
    "\n",
    "    # 6) Fobia handling: default to definition unless risk/penghidap => triggered_by\n",
    "    if 'fobia' in t:\n",
    "        if any(k in t for k in ['apa', 'tahu', 'info tentang']):\n",
    "            return 'definition'\n",
    "        if any(k in t for k in ['kondisi', 'risiko', 'penghidap']):\n",
    "            return 'triggered_by'\n",
    "        return 'definition'\n",
    "\n",
    "    # 7) related_to: check '&' or 'dan' with >=2 extracted conditions\n",
    "    if '&' in raw_title or ' dan ' in t:\n",
    "        parts = []\n",
    "        if '&' in raw_title:\n",
    "            parts = raw_title.split('&')\n",
    "        elif ' dan ' in t:\n",
    "            parts = re.split(r'\\sdan\\s', raw_title, flags=re.IGNORECASE)\n",
    "        conds = []\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            cond = extract_condition_from_title(part)\n",
    "            norm = normalize_entity(cond) if cond else ''\n",
    "            if norm:\n",
    "                conds.append(norm)\n",
    "        if len(conds) >= 2:\n",
    "            return 'related_to'\n",
    "\n",
    "    # 8) 'muncul selepas' indicates triggered_by\n",
    "    if 'muncul selepas' in t:\n",
    "        return 'triggered_by'\n",
    "\n",
    "    # 9) PTSD/Stres Pasca Trauma patterns\n",
    "    if any(x in t for x in ['ptsd', 'stres pasca trauma']):\n",
    "        if any(k in t for k in ['risiko', 'penghidap']):\n",
    "            return 'triggered_by'\n",
    "        if any(k in t for k in ['apa', 'tahu']):\n",
    "            return 'definition'\n",
    "        return 'definition'\n",
    "\n",
    "    # 10) Single condition name: treat as definition if short title without keywords\n",
    "    cond_tmp = extract_condition_from_title(raw_title)\n",
    "    norm_tmp = normalize_entity(cond_tmp) if cond_tmp else ''\n",
    "    if norm_tmp and norm_tmp in t:\n",
    "        words = re.findall(r'\\b\\w+\\b', raw_title)\n",
    "        if ':' not in raw_title and len(words) <= 6:\n",
    "            return 'definition'\n",
    "        if not any(k in t for k in ['simptom','punca','ubat','cara','rawat','tanda','gejala']):\n",
    "            return 'definition'\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94c0b7b-84b2-4ff5-827d-1db0de1e92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:02:58] INFO: Extraction complete: nodes=738, edges=657, unhandled_entries=19, definitions=70\n"
     ]
    }
   ],
   "source": [
    "unhandled_entries = []\n",
    "for idx, raw_entry in enumerate(raw_data):\n",
    "    raw_title = raw_entry.get('title', '').strip()\n",
    "    raw_content = raw_entry.get('content', '').strip()\n",
    "    clean_entry = clean_data[idx] if idx < len(clean_data) else {}\n",
    "    clean_title_tokens = []\n",
    "    if isinstance(clean_entry, dict) and 'clean_title' in clean_entry:\n",
    "        try:\n",
    "            clean_title_tokens = clean_entry.get('clean_title', [[]])[0]\n",
    "        except:\n",
    "            clean_title_tokens = []\n",
    "\n",
    "    # Skip if title or content is empty\n",
    "    if not raw_title or not raw_content:\n",
    "        unhandled_entries.append({\n",
    "            'index': idx, 'title': raw_title,\n",
    "            'reason': 'empty title or content'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # 1) Detect relation\n",
    "    relation = detect_relation(clean_title_tokens, raw_title)\n",
    "    if relation is None:\n",
    "        unhandled_entries.append({\n",
    "            'index': idx, 'title': raw_title,\n",
    "            'clean_title_tokens': clean_title_tokens,\n",
    "            'relation': None,\n",
    "            'reason': 'detect_relation returned None'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # 2) Extract condition name and normalize\n",
    "    cond_phrase = extract_condition_from_title(raw_title)\n",
    "    cond_norm = normalize_entity(cond_phrase) if cond_phrase else ''\n",
    "    if not cond_norm:\n",
    "        unhandled_entries.append({\n",
    "            'index': idx, 'title': raw_title,\n",
    "            'relation': relation,\n",
    "            'reason': f\"condition extraction empty, raw: {cond_phrase!r}\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Handle definition relation: store definition and ensure Condition node\n",
    "    if relation == 'definition':\n",
    "        if cond_norm not in condition_definitions:\n",
    "            definition_text = extract_definition(raw_content)\n",
    "            if definition_text:\n",
    "                condition_definitions[cond_norm] = definition_text\n",
    "        get_or_create_entity(cond_norm, 'Condition')\n",
    "        continue\n",
    "\n",
    "    # Handle related_to: extract multiple conditions from title and create bidirectional edges\n",
    "    if relation == 'related_to':\n",
    "        parts = []\n",
    "        if '&' in raw_title:\n",
    "            parts = raw_title.split('&')\n",
    "        elif ' dan ' in raw_title.lower():\n",
    "            parts = re.split(r'\\sdan\\s', raw_title, flags=re.IGNORECASE)\n",
    "        conds = []\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            cond = extract_condition_from_title(part)\n",
    "            norm = normalize_entity(cond) if cond else ''\n",
    "            if norm:\n",
    "                conds.append(norm)\n",
    "        if len(conds) >= 2:\n",
    "            ids = [get_or_create_entity(norm, 'Condition') for norm in conds]\n",
    "            for i in range(len(ids)):\n",
    "                for j in range(i+1, len(ids)):\n",
    "                    edges.append({\n",
    "                        'source_id': ids[i],\n",
    "                        'relation': 'related_to',\n",
    "                        'target_id': ids[j],\n",
    "                        'source_reference': raw_title\n",
    "                    })\n",
    "                    edges.append({\n",
    "                        'source_id': ids[j],\n",
    "                        'relation': 'related_to',\n",
    "                        'target_id': ids[i],\n",
    "                        'source_reference': raw_title\n",
    "                    })\n",
    "            continue\n",
    "        else:\n",
    "            unhandled_entries.append({\n",
    "                'index': idx, 'title': raw_title,\n",
    "                'relation': relation,\n",
    "                'reason': 'related_to pattern: fewer than 2 conditions extracted'\n",
    "            })\n",
    "            continue\n",
    "\n",
    "    # For has_symptom / triggered_by / treated_by, create Condition node first\n",
    "    cond_id = get_or_create_entity(cond_norm, 'Condition')\n",
    "\n",
    "    if relation == 'has_symptom':\n",
    "        patterns = [r'Simptom.*?:', r'Tanda(?: Umum)? .*?:', r'Gejala.*?:']\n",
    "    elif relation == 'triggered_by':\n",
    "        patterns = [r'Punca(?: Penyakit)? .*?:', r'Faktor .*?:', r'Picu .*?:', r'Penghidap Berisiko.*?:']\n",
    "    elif relation == 'treated_by':\n",
    "        patterns = [r'(?:Cara Mengatasi|Cara Atasi|Ubat|Rawat|Terapi|Kaunseling|Pantang Larang|Lakukan Cara Ini).*?:']\n",
    "    else:\n",
    "        patterns = []\n",
    "\n",
    "    if relation == 'triggered_by' and 'muncul selepas' in raw_title.lower():\n",
    "        m = re.search(r'Perasaan\\s+(.+?)\\s+muncul selepas\\s+(.+)', raw_title, re.IGNORECASE)\n",
    "        if m:\n",
    "            symptom_raw = m.group(1).strip()\n",
    "            cond_raw = m.group(2).strip().split('?')[0].strip()\n",
    "            for p in ['penyakit ', 'gangguan ']:\n",
    "                if cond_raw.lower().startswith(p):\n",
    "                    cond_raw = cond_raw[len(p):].strip()\n",
    "            cond_norm2 = normalize_entity(cond_raw)\n",
    "            symptom_norm = normalize_entity(symptom_raw)\n",
    "            if cond_norm2:\n",
    "                cond_id2 = get_or_create_entity(cond_norm2, 'Condition')\n",
    "                if symptom_norm:\n",
    "                    ent_id = get_or_create_entity(symptom_norm, 'Symptom')\n",
    "                    edges.append({\n",
    "                        'source_id': cond_id2,\n",
    "                        'relation': 'has_symptom',\n",
    "                        'target_id': ent_id,\n",
    "                        'source_reference': raw_title\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "    # Extract list items from content\n",
    "    items = extract_list_after_keyword(raw_content, patterns)\n",
    "    if not items:\n",
    "        unhandled_entries.append({\n",
    "            'index': idx, 'title': raw_title,\n",
    "            'relation': relation,\n",
    "            'reason': 'list extraction returned empty'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Process each extracted item: normalize, create entity node, and edge\n",
    "    for phrase in items:\n",
    "        norm = normalize_entity(phrase)\n",
    "        if not norm:\n",
    "            continue\n",
    "        if relation == 'has_symptom':\n",
    "            ent_type = 'Symptom'\n",
    "        elif relation == 'triggered_by':\n",
    "            ent_type = 'Trigger'\n",
    "        elif relation == 'treated_by':\n",
    "            # Determine Strategy vs Treatment by keywords\n",
    "            if re.search(r'\\b(teknik|relaksasi|senaman|yoga|meditasi|pantang|larang|kaunseling)\\b', norm):\n",
    "                ent_type = 'Strategy'\n",
    "            else:\n",
    "                ent_type = 'Treatment'\n",
    "        else:\n",
    "            ent_type = 'Symptom'\n",
    "        ent_id = get_or_create_entity(norm, ent_type)\n",
    "        edges.append({\n",
    "            'source_id': cond_id,\n",
    "            'relation': relation,\n",
    "            'target_id': ent_id,\n",
    "            'source_reference': raw_title\n",
    "        })\n",
    "\n",
    "logger.info(f\"Extraction complete: nodes={len(nodes)}, edges={len(edges)}, unhandled_entries={len(unhandled_entries)}, definitions={len(condition_definitions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8fdb34f-add2-4988-8c67-4776305dfffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:02:58] INFO: Saved nodes CSV: kr_output\\kg_nodes.csv\n",
      "[19:02:58] INFO: Saved edges CSV: kr_output\\kg_edges.csv\n",
      "[19:02:58] INFO: Saved unhandled entries JSON: kr_output\\unhandled_entries.json\n",
      "[19:02:58] INFO: Saved condition definitions JSON: kr_output\\condition_definitions.json\n",
      "[19:02:58] INFO: Sample nodes (first 10 rows):\n",
      "[19:02:58] INFO:     id                                                                                                                                                                                                                                                                                                          label_malay label_english      type\n",
      "  C001                                                                                                                                                                                                                                                                                                          kegelisahan               Condition\n",
      "TRT001                                                                                                                                                                                                                                                                            pernafasan dalam: tarik nafas dalam-dalam               Treatment\n",
      "TRT002                                                                                                                                                                                                                                                                                                       tahan sebentar               Treatment\n",
      "TRT003 lepaskan secara perlahan. ulangi langkah ini beberapa kali berbual dengan seseorang: berkongsi perasaan dengan orang yang anda percaya boleh meredakan kegelisahan yang dialami. anda boleh meluahkannya kepada ahli keluarga atau rakan baik anda mengatur semula fikiran anda: kenal pasti fikiran-fikiran negatif               Treatment\n",
      "STR001                                                                                                                                                              cuba tukarkannya kepada perspektif yang lebih positif lakukan senaman: aktiviti seperti berjoging atau berenang boleh mengurangkan hormon tekanan emosi                Strategy\n",
      "STR002                                                                                                                                                                  meningkatkan rasa gembira teknik relaksasi: kaedah seperti meditasi atau yoga boleh membantu menenangkan jiwa anda. bagi mereka yang beragama islam                Strategy\n",
      "TRT004                                                                                                                                                                                                                                                                                             anda boleh cuba berzikir               Treatment\n",
      "TRT005                                                                                                                                                                                                           membaca al-quran atau bersolat sunat 2 rakaat sebagai kaedah menenangkan diri kurangkan pengambilan kafein               Treatment\n",
      "STR003                                                                                                                                                                                                                                                   alkohol : kedua-dua bahan ini merupakan pantang larang kegelisahan                Strategy\n",
      "TRT006                                                                                                                                                                                                                                                                             boleh mempengaruhi kadar degupan jantung               Treatment\n",
      "[19:02:58] INFO: Sample edges (first 10 rows):\n",
      "[19:02:58] INFO: source_id   relation target_id                                                    source_reference\n",
      "     C001 treated_by    TRT001 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    TRT002 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    TRT003 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    STR001 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    STR002 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    TRT004 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    TRT005 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    STR003 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    TRT006 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "     C001 treated_by    TRT007 Cara Mengatasi Anxiety: 8 Cara Ini Boleh Bantu Anda Tenangkan Diri!\n",
      "[19:02:58] INFO: Sample unhandled entries (first 10 rows):\n",
      "[19:02:58] INFO:  index                                                                title     relation                         reason                           clean_title_tokens\n",
      "     9    Stres Pasca Trauma PTSD, Penghidap Berisiko Kena Penyakit Kronik! triggered_by list extraction returned empty                                          NaN\n",
      "    10 Obsesif Kompulsif, OCD Pun Boleh Ada Pada Kanak-Kanak,Ini Cara Atasi   treated_by list extraction returned empty                                          NaN\n",
      "    16                 claustrophobia-apa-yang-harus-anda-tahu Hello Doktor         None  detect_relation returned None              [claustrophobia, hello, doktor]\n",
      "    18     Gangguan Tekanan Selepas Trauma? Lakukan Cara Ini - Hello Doktor   treated_by list extraction returned empty                                          NaN\n",
      "    23  Penyakit OCD Yang Anda Harus Perhatikan, Jika Ada Sila Jumpa Doktor         None  detect_relation returned None         [sakit, ocd, perhati, jumpa, doktor]\n",
      "    32   Jenis Jenis Kemurungan Ada 6, Kenali Tanda-tandanya Sebelum Rawat!  has_symptom list extraction returned empty                                          NaN\n",
      "    54      strategi untuk membebaskan diri dari berfikir secara berlebihan         None  detect_relation returned None                  [strategi, bebas, berfikir]\n",
      "    62  Cara Menangani Stress: 3 Salah Faham Tentang Cara Pengurusan Stress         None  detect_relation returned None [tangan, stress, salah, faham, urus, stress]\n",
      "    64 Teknik Kawal Stres: Boleh Cuba Amalkan Teknik 4A Jika Rasa Tertekan!         None  detect_relation returned None  [teknik, kawal, stres, amal, teknik, tekan]\n",
      "    65   Hidup Gembira Boleh Dicapai Dalam Masa 30 Saat Sahaja? Ini Caranya         None  detect_relation returned None                      [hidup, gembira, capai]\n"
     ]
    }
   ],
   "source": [
    "df_nodes = pd.DataFrame(nodes)\n",
    "df_edges = pd.DataFrame(edges)\n",
    "df_unhandled = pd.DataFrame(unhandled_entries)\n",
    "\n",
    "nodes_csv = os.path.join(OUTPUT_DIR, 'kg_nodes.csv')\n",
    "edges_csv = os.path.join(OUTPUT_DIR, 'kg_edges.csv')\n",
    "unhandled_json = os.path.join(OUTPUT_DIR, 'unhandled_entries.json')\n",
    "unhandled_csv = os.path.join(OUTPUT_DIR, 'unhandled_entries.csv')\n",
    "\n",
    "df_nodes.to_csv(nodes_csv, index=False, encoding='utf-8-sig')\n",
    "df_edges.to_csv(edges_csv, index=False, encoding='utf-8-sig')\n",
    "df_unhandled.to_json(unhandled_json, force_ascii=False, orient='records', indent=2)\n",
    "df_unhandled.to_csv(unhandled_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "defs_path = os.path.join(OUTPUT_DIR, 'condition_definitions.json')\n",
    "with open(defs_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(condition_definitions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"Saved nodes CSV: {nodes_csv}\")\n",
    "logger.info(f\"Saved edges CSV: {edges_csv}\")\n",
    "logger.info(f\"Saved unhandled entries JSON: {unhandled_json}\")\n",
    "logger.info(f\"Saved condition definitions JSON: {defs_path}\")\n",
    "logger.info(\"Sample nodes (first 10 rows):\")\n",
    "logger.info(df_nodes.head(10).to_string(index=False))\n",
    "logger.info(\"Sample edges (first 10 rows):\")\n",
    "logger.info(df_edges.head(10).to_string(index=False))\n",
    "logger.info(\"Sample unhandled entries (first 10 rows):\")\n",
    "logger.info(df_unhandled.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3cb9cef-d80c-404b-b798-6029b5077d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:02:59] INFO: OWL ontology saved to: kr_output\\mental_health_ontology.ttl\n"
     ]
    }
   ],
   "source": [
    "if HAVE_RDFLIB:\n",
    "    df_nodes2 = pd.read_csv(nodes_csv, encoding='utf-8-sig')\n",
    "    df_edges2 = pd.read_csv(edges_csv, encoding='utf-8-sig')\n",
    "    with open(defs_path, 'r', encoding='utf-8') as f:\n",
    "        cond_defs = json.load(f)\n",
    "\n",
    "    BASE = Namespace('http://example.org/mentalhealth#')\n",
    "    g = Graph()\n",
    "    g.bind('mh', BASE)\n",
    "    g.bind('rdfs', RDFS)\n",
    "    g.bind('owl', OWL)\n",
    "\n",
    "    # Define Classes: Condition, Symptom, Trigger, Treatment, Strategy, Professional\n",
    "    for ent_type in df_nodes2['type'].unique():\n",
    "        class_uri = BASE[ent_type]\n",
    "        g.add((class_uri, RDF.type, OWL.Class))\n",
    "        g.add((class_uri, RDFS.label, Literal(ent_type, lang='en')))\n",
    "\n",
    "    # Define ObjectProperties\n",
    "    relation_defs = {\n",
    "        'has_symptom': ('Condition', 'Symptom'),\n",
    "        'triggered_by': ('Condition', 'Trigger'),\n",
    "        'treated_by': ('Condition', 'Treatment'),\n",
    "        'related_to': ('Condition', 'Condition'),\n",
    "    }\n",
    "    for rel, (dom, rng) in relation_defs.items():\n",
    "        prop = BASE[rel]\n",
    "        g.add((prop, RDF.type, OWL.ObjectProperty))\n",
    "        g.add((prop, RDFS.domain, BASE[dom]))\n",
    "        g.add((prop, RDFS.range, BASE[rng]))\n",
    "        g.add((prop, RDFS.label, Literal(rel, lang='en')))\n",
    "\n",
    "    # Add individuals with labels and definitions\n",
    "    for _, row in df_nodes2.iterrows():\n",
    "        node_uri = BASE[row['id']]\n",
    "        ent_type = row['type']\n",
    "        g.add((node_uri, RDF.type, BASE[ent_type]))\n",
    "        mal_label = row['label_malay']\n",
    "        if isinstance(mal_label, str) and mal_label:\n",
    "            g.add((node_uri, RDFS.label, Literal(mal_label, lang='ms')))\n",
    "        eng_label = row.get('label_english', '')\n",
    "        if isinstance(eng_label, str) and eng_label:\n",
    "            g.add((node_uri, RDFS.label, Literal(eng_label, lang='en')))\n",
    "        if ent_type == 'Condition' and mal_label in cond_defs:\n",
    "            definition_text = cond_defs.get(mal_label)\n",
    "            if definition_text:\n",
    "                g.add((node_uri, RDFS.comment, Literal(definition_text, lang='ms')))\n",
    "\n",
    "    # Add relation triples\n",
    "    for _, row in df_edges2.iterrows():\n",
    "        subj = BASE[row['source_id']]\n",
    "        obj = BASE[row['target_id']]\n",
    "        prop = BASE[row['relation']]\n",
    "        g.add((subj, prop, obj))\n",
    "\n",
    "    # Serialize to Turtle\n",
    "    owl_path = os.path.join(OUTPUT_DIR, 'mental_health_ontology.ttl')\n",
    "    g.serialize(destination=owl_path, format='turtle')\n",
    "    logger.info(f\"OWL ontology saved to: {owl_path}\")\n",
    "else:\n",
    "    logger.warning(\"rdflib not installed, skipping OWL generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934788d1-9a39-47a9-bc22-b14dcbb25c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_dict():\n",
    "    df_n = pd.read_csv(nodes_csv, encoding='utf-8-sig')\n",
    "    df_e = pd.read_csv(edges_csv, encoding='utf-8-sig')\n",
    "    id_to_label = dict(zip(df_n['id'], df_n['label_malay']))\n",
    "    graph = {}\n",
    "    for _, row in df_e.iterrows():\n",
    "        subj = id_to_label.get(row['source_id'], '')\n",
    "        tgt = id_to_label.get(row['target_id'], '')\n",
    "        if not subj or not tgt:\n",
    "            continue\n",
    "        graph.setdefault(subj, {}).setdefault(row['relation'], []).append(tgt)\n",
    "    return graph\n",
    "\n",
    "graph = build_graph_dict()\n",
    "with open(defs_path, 'r', encoding='utf-8') as f:\n",
    "    condition_definitions = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
